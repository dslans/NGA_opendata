### Project Plan: AI Museum Curator

**1. Project Overview**

The goal of this project is to create an AI-powered application that acts as a virtual museum curator. Users will provide a theme, and the application will select and display a collection of relevant artworks from the National Gallery of Art's open data collection.

**2. Required Datasets**

We will primarily use the following CSV files from the NGA dataset, which are all loaded into BigQuery:

*   **objects.csv**: The core table, containing information about each art object.
*   **constituents.csv**: Information about the creators of the art objects.
*   **objects_constituents.csv**: A mapping table that links objects to their constituents.
*   **published_images.csv**: Information about the images of the art objects.
*   **locations.csv**: The physical locations of the art objects.
*   **alternative_identifiers.csv**: Alternative identifiers for objects and constituents.
*   **constituents_altnames.csv**: Alternative names for the constituents.
*   **constituents_text_entries.csv**: Bibliographical and other textual information about the constituents.
*   **media_items.csv**: Audio and video items related to the collection.
*   **media_relationships.csv**: Links between media items and the art objects or constituents they relate to.
*   **object_associations.csv**: Relationships between art objects.
*   **objects_dimensions.csv**: Detailed dimensions for the art objects.
*   **objects_historical_data.csv**: Historical data about the objects.
*   **objects_terms.csv**: Keywords, themes, and other terms associated with the objects.
*   **objects_text_entries.csv**: Exhibition histories, provenance, and other textual information about the objects.
*   **preferred_locations.csv**: Public-facing location descriptions.
*   **preferred_locations_tms_locations.csv**: A mapping between the internal location IDs and the public-facing location descriptions.

**3. Required Tooling and Technologies**

*   **Programming Language:** Python 3
*   **Data Manipulation:** The `pandas` library for reading and filtering the CSV data.
*   **AI and Language Model:** Google Cloud Vertex AI for processing user prompts.
*   **Web Application Framework:** `Streamlit` to create a simple and interactive user interface for the application.
*   **Testing:** `pytest` for running unit and integration tests.
*   **Dependencies:**
    *   `pandas`
    *   `streamlit`
    *   `google-cloud-aiplatform`
    *   `google-cloud-bigquery`
    *   `python-dotenv`
    *   `pytest`
    *   `pytest-html`

**4. Project Structure**

```
/nga-curator/
|-- app.py
|-- data/
|   |-- objects.csv
|   |-- objects_terms.csv
|   |-- published_images.csv
|   |-- constituents.csv
|-- requirements.txt
|-- scripts/
|   `-- run_data_quality_tests.sh
|   `-- load_to_bigquery.py
|   `-- bigquery_utils.py
|-- tests/
|   |-- reports/
|   `-- test_data_quality.py
```

**5. Application Workflow**

1.  **One-Time Data Loading with `load_to_bigquery.py`:**
    *   This script is responsible for creating the `nga_open_data` BigQuery dataset and uploading the cleaned data from all 17 CSV files.
    *   Column names are sanitized to be BigQuery-compatible (e.g., replacing spaces and special characters with underscores).
    *   The script then uploads the processed data into their respective tables in BigQuery.

2.  **Application Startup:**
    *   The Streamlit app (`app.py`) will initialize a connection to Google Cloud BigQuery and offer enhanced search capabilities across all metadata fields (titles, terms, artist names, medium, classification).

3.  **User Input:**
    *   The user will be prompted to enter a theme in a text box.

4.  **Theme Analysis with Vertex AI:**
    *   The user's theme will be sent to Vertex AI to extract a list of relevant search keywords.

5.  **Artwork Selection via BigQuery:**
    *   The application will construct a SQL query to run against the BigQuery tables.
    *   This query will `JOIN` the necessary tables and use a `WHERE` clause to find artworks where the `term` or `title` matches the keywords generated by the AI.

6.  **Display Results:**
    *   The results of the BigQuery query will be displayed in the Streamlit app, showing detailed artwork information including images, titles, artists, provenance, exhibition history, and related artworks.

7.  **Testing with `pytest`:**
    *   The project includes a `tests/` directory with tests for data quality and a `scripts/run_data_quality_tests.sh` script to execute them.
    *   Running the script generates a `test_data_load_report.html` for immediate review and also saves a timestamped copy to the `tests/reports/` directory to maintain a history of data quality over time.

**6. Infrastructure Requirements (Terraform)**

The following Google Cloud infrastructure is provisioned and managed using Terraform:

*   **Service Account (`nga-curator-sa`):** A dedicated service account is created for the AI Museum Curator application to interact with Google Cloud services.
    *   **Roles and Permissions:**
        *   `roles/bigquery.dataEditor`: Provides comprehensive permissions for managing BigQuery data, including reading, writing, and updating datasets and tables. This role ensures the application can query the NGA open data.
        *   `roles/bigquery.user`: Grants permissions to run BigQuery jobs, including queries and read sessions. This is crucial for the application to execute queries against BigQuery.
        *   `roles/aiplatform.user`: Allows the service account to interact with Vertex AI services, specifically for using generative models to analyze user themes.
*   **Terraform Configuration:** The infrastructure is defined in the `main.tf` file at the project root. This file includes the service account definition, IAM role bindings, and service account key creation.
