### Project Plan: AI Museum Curator

**1. Project Overview**

The goal of this project is to create an AI-powered application that acts as a virtual museum curator. Users will provide a theme, and the application will select and display a collection of relevant artworks from the National Gallery of Art's open data collection.

**2. Required Datasets**

We will primarily use the following CSV files from the NGA dataset:

*   `objects.csv`: The main dataset with details about each artwork.
*   `objects_terms.csv`: Contains keywords and themes associated with the artworks, which will be crucial for matching themes.
*   `published_images.csv`: Provides URLs to the images of the artworks for display.
*   `constituents.csv`: To provide more context on the artists.

**3. Required Tooling and Technologies**

*   **Programming Language:** Python 3
*   **Data Manipulation:** The `pandas` library for reading and filtering the CSV data.
*   **AI and Language Model:** Google Cloud Vertex AI for processing user prompts.
*   **Web Application Framework:** `Streamlit` to create a simple and interactive user interface for the application.
*   **Testing:** `pytest` for running unit and integration tests.
*   **Dependencies:**
    *   `pandas`
    *   `streamlit`
    *   `google-cloud-aiplatform`
    *   `google-cloud-bigquery`
    *   `python-dotenv`
    *   `pytest`
    *   `pytest-html`

**4. Project Structure**

```
/nga-curator/
|-- app.py
|-- load_to_bigquery.py
|-- data/
|   |-- objects.csv
|   |-- objects_terms.csv
|   |-- published_images.csv
|   `-- constituents.csv
|-- requirements.txt
|-- scripts/
|   `-- run_data_quality_tests.sh
|-- tests/
|   |-- reports/
|   `-- test_data_quality.py
```

**5. Application Workflow**

1.  **One-Time Data Loading with `load_to_bigquery.py`:**
    *   This script is responsible for creating the `nga_open_data` BigQuery dataset and uploading the cleaned data from the CSV files.
    *   It reads the core CSV files (`objects`, `objects_terms`, `published_images`, `constituents`, and `objects_constituents`).
    *   Column names are sanitized to be BigQuery-compatible (e.g., replacing spaces and special characters with underscores).
    *   The script then uploads the processed data into their respective tables in BigQuery.

2.  **Application Startup:**
    *   The Streamlit app (`app.py`) will initialize a connection to Google Cloud BigQuery.

3.  **User Input:**
    *   The user will be prompted to enter a theme in a text box.

4.  **Theme Analysis with Vertex AI:**
    *   The user's theme will be sent to Vertex AI to extract a list of relevant search keywords.

5.  **Artwork Selection via BigQuery:**
    *   The application will construct a SQL query to run against the BigQuery tables.
    *   This query will `JOIN` the necessary tables and use a `WHERE` clause to find artworks where the `term` or `title` matches the keywords generated by the AI.

6.  **Display Results:**
    *   The results of the BigQuery query will be displayed in the Streamlit app, showing the artwork images, titles, artists, and other relevant information.

7.  **Testing with `pytest`:**
    *   The project includes a `tests/` directory with tests for data quality and a `scripts/run_data_quality_tests.sh` script to execute them.
    *   Running the script generates a `test_data_load_report.html` for immediate review and also saves a timestamped copy to the `tests/reports/` directory to maintain a history of data quality over time.

**6. Infrastructure Requirements (Terraform)**

The following Google Cloud infrastructure is provisioned and managed using Terraform:

*   **Service Account (`nga-curator-sa`):** A dedicated service account is created for the AI Museum Curator application to interact with Google Cloud services.
    *   **Roles and Permissions:**
        *   `roles/bigquery.dataEditor`: Provides comprehensive permissions for managing BigQuery data, including reading, writing, and updating datasets and tables. This role ensures the application can query the NGA open data.
        *   `roles/bigquery.user`: Grants permissions to run BigQuery jobs, including queries and read sessions. This is crucial for the application to execute queries against BigQuery.
        *   `roles/aiplatform.user`: Allows the service account to interact with Vertex AI services, specifically for using generative models to analyze user themes.
*   **Terraform Configuration:** The infrastructure is defined in the `main.tf` file at the project root. This file includes the service account definition, IAM role bindings, and service account key creation.
